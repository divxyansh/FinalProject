# -*- coding: utf-8 -*-
"""baseclassifiers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15TrdpCHeWqvewNr2D0-EqDKVdZMc803y
"""

from google.colab import files
uploaded = files.upload()
print(uploaded)

# Install necessary libraries
# Use these commands in terminal or command prompt:
# pip install imbalanced-learn
# pip install tensorflow
# pip install pandas scikit-learn
# pip install scikeras

# for colab
# Install necessary libraries
#!pip install imbalanced-learn --quiet
#!pip install tensorflow --quiet
#!pip install pandas scikit-learn --quiet
#!pip install scikeras --quiet

# Import required libraries
from sklearn.ensemble import StackingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler

# Define classifiers
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import SGDClassifier

# Load dataset
df = pd.read_csv('ML-EdgeIIoT-dataset.csv')
df.dropna(inplace=True)
df.drop_duplicates(inplace=True)

#print(df.shape)
#print(df.describe())

# Data preprocessing
# Separate features and target

print("PREPROCESSING")
X = df.drop('Attack_type', axis=1)
y = df['Attack_type']

# Encode target labels
if y.dtype == 'object':
    le_y = LabelEncoder()
    y = le_y.fit_transform(y)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Handle non-numeric features
non_numeric_cols = X_train.select_dtypes(include=['object']).columns
for column in non_numeric_cols:
    if column == 'frame.time':
        try:
            X_train[column] = pd.to_datetime(X_train[column], errors='coerce')
            X_test[column] = pd.to_datetime(X_test[column], errors='coerce')
            X_train[column] = X_train[column].astype('int64') // 10**9
            X_test[column] = X_test[column].astype('int64') // 10**9
        except Exception as e:
            print(f"Dropping problematic column: {column}")
            X_train.drop(column, axis=1, inplace=True)
            X_test.drop(column, axis=1, inplace=True)
    else:
        le = LabelEncoder()
        combined = pd.concat([X_train[column], X_test[column]], axis=0).astype(str)
        le.fit(combined)
        X_train[column] = le.transform(X_train[column].astype(str))
        X_test[column] = le.transform(X_test[column].astype(str))

# Fill missing values
X_train.fillna(0, inplace=True)
X_test.fillna(0, inplace=True)

# Feature scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Define base classifiers
base_learners = [
    ('Logistic Regression', LogisticRegression(max_iter=1000, random_state=42)),
    ('K-NN', KNeighborsClassifier(n_neighbors=5)),
    ('Naive Bayes', GaussianNB()),
    ('MLP', MLPClassifier(max_iter=500))
]

#    ('Decision Tree', DecisionTreeClassifier(random_state=42))
#    ('svm', SVC(probability=True, kernel='rbf', random_state=42))

# Print accuracy and per-class report for each base classifier
print("Base Classifier Performance:\n")
for name, model in base_learners:
    print(f"\nBase Classifier Performance: {name}")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    acc = accuracy_score(y_test, y_pred)
#    print(f"\n Base Classifier: {name} - Accuracy: {acc:.4f}")
    print("Classification Report:")
    # Get classification report as dict
    report = classification_report(y_test, y_pred, target_names=le_y.classes_, output_dict=True)

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    labels = le_y.classes_

    # Plot confusion matrix
    fig, ax = plt.subplots(figsize=(12, 10))  # Adjust size as needed
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
    disp.plot(cmap='Blues', ax=ax, colorbar=True)

    # Fix x-axis label alignment
    ax.set_xticks(np.arange(len(labels)))
    ax.set_xticklabels(labels, rotation=45, ha='right')  # Rotate and align
    plt.title(f'Confusion Matrix - {name}')
    plt.tight_layout()
    plt.savefig(f"{name}_confusion_matrix.tiff", dpi=300, format='tiff')
    plt.show()

    # Initialize list to collect per-class metrics
    rows = []

    # Collect FPRs, FNRs, and supports
    fprs = []
    fnrs = []
    supports = []

    for i, label in enumerate(labels):
      TP = cm[i, i]
      FN = cm.sum(axis=1)[i] - TP
      FP = cm.sum(axis=0)[i] - TP
      TN = cm.sum() - (TP + FP + FN)

      precision = report[label]["precision"]
      recall = report[label]["recall"]
      f1 = report[label]["f1-score"]
      support = report[label]["support"]

      fpr = FP / (FP + TN) if (FP + TN) > 0 else 0
      fnr = FN / (FN + TP) if (FN + TP) > 0 else 0

      fprs.append(fpr)
      fnrs.append(fnr)
      supports.append(support)

      rows.append({
        "Class": label,
        "Precision": round(precision, 4),
        "Recall": round(recall, 4),
        "F1-Score": round(f1, 4),
        "Support": int(support),
        "FPR": round(fpr, 4),
        "FNR": round(fnr, 4)
      })


    # Compute macro and weighted averages for FPR and FNR
    macro_fpr = np.mean(fprs)
    macro_fnr = np.mean(fnrs)
    weighted_fpr = np.average(fprs, weights=supports)
    weighted_fnr = np.average(fnrs, weights=supports)

    # Add empty row
    rows.append({
        "Class": " ",
        "Precision": " ",
        "Recall": " ",
        "F1-Score": " ",
        "Support": " ",
        "FPR": " ",
        "FNR": " "
    })

    # Add accuracy row
    rows.append({
      "Class": "accuracy",
      "Precision": "-",
      "Recall": "-",
      "F1-Score": round(acc,4),
      "Support": int(report['accuracy'] * len(y_test)),
      "FPR": "-",
      "FNR": "-"
    })

    # Add macro avg row
    rows.append({
        "Class": "macro avg",
        "Precision": round(report['macro avg']["precision"], 4),
        "Recall": round(report['macro avg']["recall"], 4),
        "F1-Score": round(report['macro avg']["f1-score"], 4),
        "Support": int(report['macro avg']["support"]),
        "FPR": round(macro_fpr, 4),
        "FNR": round(macro_fnr, 4)
    })

    # Add weighted avg row
    rows.append({
        "Class": "weighted avg",
        "Precision": round(report['weighted avg']["precision"], 4),
        "Recall": round(report['weighted avg']["recall"], 4),
        "F1-Score": round(report['weighted avg']["f1-score"], 4),
        "Support": int(report['weighted avg']["support"]),
        "FPR": round(weighted_fpr, 4),
        "FNR": round(weighted_fnr, 4)
    })


    '''
    # Add macro avg, weighted avg
    for avg in ['macro avg', 'weighted avg']:
      precision = report[avg]["precision"]
      recall = report[avg]["recall"]
      f1 = report[avg]["f1-score"]
      support = report[avg]["support"]
      rows.append({
        "Class": avg,
        "Precision": round(precision, 4),
        "Recall": round(recall, 4),
        "F1-Score": round(f1, 4),
        "Support": int(support),
        "FPR": "-",  # Not meaningful for macro/weighted
        "FNR": "-"
      })
    '''
    # Create DataFrame
    results_df = pd.DataFrame(rows)

    # Print table
    print("Evaluation Metrics per Class:")
    print(results_df.to_string(index=False))

    # Save to CSV
    results_df.to_csv(name+'_classifier_summary.csv', index=False)
    print(f"\n Results saved to ' {name}_classifier_summary.csv'")